{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statement</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>For the first time in more than a decade, impo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>Says Donald Trump has bankrupted his companies...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2548</th>\n",
       "      <td>John McCain and George Bush have \"absolutely n...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>A new poll shows 62 percent support the presid...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>No one claims the report vindicating New Jerse...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12791 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Statement  Label\n",
       "0     Says the Annies List political group supports ...  False\n",
       "1     When did the decline of coal start? It started...   True\n",
       "2     Hillary Clinton agrees with John McCain \"by vo...   True\n",
       "3     Health care reform legislation is likely to ma...  False\n",
       "4     The economic turnaround started at the end of ...   True\n",
       "...                                                 ...    ...\n",
       "2546  For the first time in more than a decade, impo...   True\n",
       "2547  Says Donald Trump has bankrupted his companies...   True\n",
       "2548  John McCain and George Bush have \"absolutely n...   True\n",
       "2549  A new poll shows 62 percent support the presid...  False\n",
       "2550  No one claims the report vindicating New Jerse...  False\n",
       "\n",
       "[12791 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#餵入資料\n",
    "import pandas as pd\n",
    "\n",
    "test_filename = 'test.csv'\n",
    "train_filename = 'train.csv'\n",
    "\n",
    "train_news = pd.read_csv(train_filename)\n",
    "test_news = pd.read_csv(test_filename)\n",
    "df=pd.concat([train_news,test_news])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#資料預處理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk.data\n",
    "import logging \n",
    "import gensim\n",
    "\n",
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "        text = text.split()\n",
    "        stemmer=SnowballStemmer('english')\n",
    "        stemmed_words=[stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        return text\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "  \n",
    "# Clean the summaries and texts\n",
    "clean_texts = []\n",
    "for text in df.Statement:\n",
    "    clean_texts.append(clean_text(text))\n",
    "\n",
    "print(\"Texts are complete.\")\n",
    "\n",
    "print (len(clean_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Statement'] = df['Statement'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "print(len(train),len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer&tfidf多種分類比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "countV = CountVectorizer()\n",
    "tfidf=TfidfTransformer()\n",
    "\n",
    "countV_tfidf = Pipeline([\n",
    "        ('NBCV',countV),\n",
    "        ('tfidf',tfidf)])\n",
    "\n",
    "countV_tfidf.fit(df['Statement'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Statement =  countV_tfidf.transform(train['Statement'].values)\n",
    "test_Statement =  countV_tfidf.transform(test['Statement'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#building classifier using naive bayes \n",
    "nb=MultinomialNB()\n",
    "nb.fit(train_Statement,train['Label'])\n",
    "predicted_nb = nb.predict(test_Statement)\n",
    "np.mean(predicted_nb == test['Label'])\n",
    "\n",
    "#building classifier using logistic regression\n",
    "logR=LogisticRegression()\n",
    "logR.fit(train_Statement,train['Label'])\n",
    "predicted_LogR = logR.predict(test_Statement)\n",
    "np.mean(predicted_LogR == test['Label'])\n",
    "\n",
    "#building Linear SVM classfier\n",
    "SVM=svm.LinearSVC()\n",
    "SVM.fit(train_Statement,train['Label'])\n",
    "predicted_SVM = SVM.predict(test_Statement)\n",
    "np.mean(predicted_SVM == test['Label'])\n",
    "\n",
    "#using SVM Stochastic Gradient Descent on hinge loss\n",
    "SGD=SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5)\n",
    "SGD.fit(train_Statement,train['Label'])\n",
    "predicted_SGD = SGD.predict(test_Statement)\n",
    "np.mean(predicted_SGD == test['Label'])\n",
    "\n",
    "#random forest\n",
    "random_forest=RandomForestClassifier(n_estimators=200,n_jobs=3)\n",
    "random_forest.fit(train_Statement,train['Label'])\n",
    "predicted_rf = random_forest.predict(test_Statement)\n",
    "np.mean(predicted_rf == test['Label'])\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('NB Testing accuracy %s' % accuracy_score(test['Label'], predicted_nb))\n",
    "print('NB Testing F1 score: {}'.format(f1_score(test['Label'], predicted_nb, average='weighted')))\n",
    "print()\n",
    "print('LogR Testing accuracy %s' % accuracy_score(test['Label'], predicted_LogR))\n",
    "print('LogR Testing F1 score: {}'.format(f1_score(test['Label'], predicted_LogR, average='weighted')))\n",
    "print()\n",
    "print('SVM Testing accuracy %s' % accuracy_score(test['Label'], predicted_SVM))\n",
    "print('SVM Testing F1 score: {}'.format(f1_score(test['Label'], predicted_SVM, average='weighted')))\n",
    "print()\n",
    "print('SGD Testing accuracy %s' % accuracy_score(test['Label'], predicted_SGD))\n",
    "print('SGD Testing F1 score: {}'.format(f1_score(test['Label'], predicted_SGD, average='weighted')))\n",
    "print()\n",
    "print('RF Testing accuracy %s' % accuracy_score(test['Label'], predicted_rf))\n",
    "print('RF Testing F1 score: {}'.format(f1_score(test['Label'], predicted_rf, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf_ngram多種分類比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_ngram = TfidfVectorizer(stop_words='english',ngram_range=(1,4),use_idf=True,smooth_idf=True)\n",
    "tfidf_ngram.fit(df['Statement'])\n",
    "train_ngram =  countV_tfidf.transform(train['Statement'].values)\n",
    "test_ngram =  countV_tfidf.transform(test['Statement'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building classifier using naive bayes \n",
    "nb=MultinomialNB()\n",
    "nb.fit(train_ngram,train['Label'])\n",
    "predicted_nb_ngram = nb.predict(test_ngram)\n",
    "np.mean(predicted_nb_ngram == test['Label'])\n",
    "\n",
    "#building classifier using logistic regression\n",
    "logR=LogisticRegression()\n",
    "logR.fit(train_ngram,train['Label'])\n",
    "predicted_LogR_ngram = logR.predict(test_ngram)\n",
    "np.mean(predicted_LogR_ngram == test['Label'])\n",
    "\n",
    "#building Linear SVM classfier\n",
    "SVM=svm.LinearSVC()\n",
    "SVM.fit(train_ngram,train['Label'])\n",
    "predicted_SVM_ngram = SVM.predict(test_ngram)\n",
    "np.mean(predicted_SVM_ngram == test['Label'])\n",
    "\n",
    "#using SVM Stochastic Gradient Descent on hinge loss\n",
    "SGD=SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5)\n",
    "SGD.fit(train_ngram,train['Label'])\n",
    "predicted_SGD_ngram = SGD.predict(test_ngram)\n",
    "np.mean(predicted_SGD_ngram == test['Label'])\n",
    "\n",
    "#random forest\n",
    "random_forest=RandomForestClassifier(n_estimators=200,n_jobs=3)\n",
    "random_forest.fit(train_ngram,train['Label'])\n",
    "predicted_rf_ngram = random_forest.predict(test_ngram)\n",
    "np.mean(predicted_rf_ngram == test['Label'])\n",
    "\n",
    "print('NB_ngram Testing accuracy %s' % accuracy_score(test['Label'], predicted_nb_ngram))\n",
    "print('NB_ngram Testing F1 score: {}'.format(f1_score(test['Label'], predicted_nb, average='weighted')))\n",
    "print()\n",
    "print('LogR_ngram Testing accuracy %s' % accuracy_score(test['Label'], predicted_LogR_ngram))\n",
    "print('LogR_ngram Testing F1 score: {}'.format(f1_score(test['Label'], predicted_LogR, average='weighted')))\n",
    "print()\n",
    "print('SVM_ngram Testing accuracy %s' % accuracy_score(test['Label'], predicted_SVM_ngram))\n",
    "print('SVM_ngram Testing F1 score: {}'.format(f1_score(test['Label'], predicted_SVM, average='weighted')))\n",
    "print()\n",
    "print('SGD_ngram Testing accuracy %s' % accuracy_score(test['Label'], predicted_SGD_ngram))\n",
    "print('SGD_ngram Testing F1 score: {}'.format(f1_score(test['Label'], predicted_SGD, average='weighted')))\n",
    "print()\n",
    "print('RF_ngram Testing accuracy %s' % accuracy_score(test['Label'], predicted_rf_ngram))\n",
    "print('RF_ngram Testing F1 score: {}'.format(f1_score(test['Label'], predicted_rf, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec多種分類比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['Statement']), tags=[r.Label]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['Statement']), tags=[r.Label]), axis=1)\n",
    "train_tagged.values[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "from gensim.models import Doc2Vec\n",
    "from tqdm import tqdm\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import utils\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Final Vector Feature for the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building classifier using logistic regression\n",
    "logR=LogisticRegression()\n",
    "logR.fit(X_train,y_train)\n",
    "predicted_logR_d2v = logR.predict(X_test)\n",
    "\n",
    "#building Linear SVM classfier\n",
    "SVM=svm.LinearSVC()\n",
    "SVM.fit(X_train,y_train)\n",
    "predicted_SVM_d2v = SVM.predict(X_test)\n",
    "\n",
    "#using SVM Stochastic Gradient Descent on hinge loss\n",
    "SGD=SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5)\n",
    "SGD.fit(X_train,y_train)\n",
    "predicted_SGD_d2v = SGD.predict(X_test)\n",
    "\n",
    "#random forest\n",
    "random_forest=RandomForestClassifier(n_estimators=200,n_jobs=3)\n",
    "random_forest.fit(X_train,y_train)\n",
    "predicted_rf_d2v = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LogR_d2v Testing accuracy %s' % accuracy_score(y_test, predicted_logR_d2v))\n",
    "print('LogR_d2v Testing F1 score: {}'.format(f1_score(y_test, predicted_logR_d2v, average='weighted')))\n",
    "print()\n",
    "print('SVM_d2v Testing accuracy %s' % accuracy_score(y_test, predicted_SVM_d2v))\n",
    "print('SVM_d2v Testing F1 score: {}'.format(f1_score(y_test, predicted_SVM_d2v, average='weighted')))\n",
    "print()\n",
    "print('SGD_d2v Testing accuracy %s' % accuracy_score(y_test, predicted_SGD_d2v))\n",
    "print('SGD_d2v Testing F1 score: {}'.format(f1_score(y_test, predicted_SGD_d2v, average='weighted')))\n",
    "print()\n",
    "print('RF_d2v Testing accuracy %s' % accuracy_score(y_test, predicted_rf_d2v))\n",
    "print('RF_d2v Testing F1 score: {}'.format(f1_score(y_test, predicted_rf_d2v, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
